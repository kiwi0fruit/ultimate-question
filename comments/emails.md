Dear Carson C. Chow,

> <...> I suggest that coming up with a well-defined set of constraints it would be crucial to your quest.  What exactly do you mean by Laws of Nature?  Do you mean the Laws of Nature we observe or any set of laws?  What do you mean by simplest?  Lowest Kolmogorov complexity? Fewest number of rules?
>
> You need to define these concepts precisely. <...>

By pointing to that particular constraints you actually listed the two hardest problems I struggled and haven't solved. And even more: that two problems seem to be two sides of the same single problem.

From simple point of view we should not really expect to have **all** laws of nature from our particular real universe if we are to discover laws via natural selection modelling. The laws of nature should be populations of individuals that share quite similar features (like "genes"). That populations slowly change in time but we are to expect them to change **gradually** and **slowly** so if we are to examine/investigate simulation of the desired mathematical model we can distinguish periods when population features are more or less stable. That populations also do not stop their evolution on some fixed level of complexity - they should be capable of producing complex "intelligent" agents after some presumably **great** time (to actually get intelligent agents in simulation is impossible with our limited abilities).

But from the second point of view we should define laws of nature in a common way: the laws (meta-laws?) that govent natural selection and provide postulates of natural selection to the model. And this is where we get metaphysics involved with Occam's razor and the question of "What is the simplest?". The guiding constraint of my research program is that the desired model should be the simplest to the point of being self-justifying. That's important because I only have metaphysics philosophy considerations for this. And philosophy considerations should be **obvious** (or close to it) to be useful. For example from philosophy point of view we can argue that the Universe should've had the simplest first state at the first moment of time (1: the World wasn't created this morning with me unshaven, 2: we do not hide behind infinite causal chain that goes back to the past like ancient Greek hide gravitation pull to the Earth behind infinite elephants that the great turtle rest on). And Occam's razor presumption is quite good for justifying it. That's good but drastically not enough to build a math model.

So we need to define that **meta-laws that simple to the point of being self-justifying**... It's still possible to go with for no reason particular laws that are not self-justifying but with this we lose the only way of pure philosophy justifying. And we also already do not have empirical justifying because if we are to go with natural selection thing then we should think that Smolin's fecund universes hypothesis (https://en.wikipedia.org/wiki/Lee_Smolin#Cosmological_natural_selection) is likely to be true. And that means that our observable universe could have had a very large number of universes-ancestors. So our real universe would definitely be out of our modelling abilities even if we have the desired model formulated. So we have no choise but to define that **meta-laws that simple to the point of being self-justifying**...

I have an intuitive feeling that some intuitive concepts like causation, recursion, reproduction/doubling, randomness/spontaneous symmetry breaking, natural selection can be obvious and self-justifying to be at that simple first state but at the moment my thoughts about them is more like a mess than something useful... The part that saddens me most is the desire to get rid of meta-laws completely and to "incapsulate" them into the graph-like structure (whatever that means...), to make them "immanent"...

By the way. This whole task mostly is in the metaphysics and math intersection. The one to accomplish it should be serious about both 1) precise math definitions and processes descriptiopns and 2) metaphysics problems and reasoning. If one to ignore, belittle or downplay either of two components then he is unlikely to succed in this particular direction. May be you know where I can find people that are serious about both these factors? (Most philosophers I come across do not tend to provide math models for their speculations... Most mathematicians do not care about metaphysics except platonism... Most physicists think that metaphysics is a bullshit...)

> <...> I think self justification can never be constrained enough because it relies on your sense of aesthetic.  You need a machine to decide. <...>

You raise a good point that self-justification based on my own sense of what should be done (aesthetics of wetvare trained neural network) is not enough to properly define the initial hardcoded laws that provide medium for natural selection to work on.

I believe some of that aesthetics are good to go and should not be revised as they are *enough* self-justified: discrete time and space, the first "simplest" state at the first moment of time, incorporation of natural selection postulates in some way. But what means "simplest" and how to include natural selection are ill-defined so it's the target of research.

Rest of my aeatetics try to guide me how to add that, but with problems... Actually there might be another way: to solve "open-ended evolution" problem (https://kiwi0fruit.github.io/ultimate-question/ultimate_question.html#s3) by chance or blind variation (or any other way). Then try to strip as much from the model as possible and see if it's can be viewed as self-justifying from philosophical point of view... But it's a risky way as it's likely that we would not be able to strip enough to justify.
