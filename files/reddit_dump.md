## Toward a theory of evolution as multilevel learning

Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin

#### Abstract

We apply the theory of learning to physically renormalizable systems in an attempt to outline a theory of biological evolution, including the origin of life, as multilevel learning. We formulate seven fundamental principles of evolution that appear to be necessary and sufficient to render a universe observable and show that they entail the major features of biological evolution, including replication and natural selection. It is shown that these cornerstone phenomena of biology emerge from the fundamental features of learning dynamics such as the existence of a loss function, which is minimized during learning. We then sketch the theory of evolution using the mathematical framework of neural networks, which provides for detailed analysis of evolutionary phenomena. To demonstrate the potential of the proposed theoretical framework, we derive a generalized version of the Central Dogma of molecular biology by analyzing the flow of information during learning (back propagation) and predicting (forward propagation) the environment by evolving organisms. The more complex evolutionary phenomena, such as major transitions in evolution (in particular, the origin of life), have to be analyzed in the thermodynamic limit, which is described in detail in the paper by Vanchurin et al.

https://pubmed.ncbi.nlm.nih.gov/35121666/

(free access)

----

## [Dmitry Shabanov: On the immortality of populations (autotranslated)](https://old-computerra-ru.translate.goog/own/shabanov/612801/?_x_tr_sl=ru&_x_tr_tl=en&_x_tr_hl=ru)

Original: https://old.computerra.ru/own/shabanov/612801/

----

## What are the biggest questions in ALife? My take: are quantum computing capabilities essential to get open-ended evolution?

More specificaly it's what ontology is enough to get [open-ended evolution](https://www.reddit.com/r/oee/). I have no idea... My best guess is to apply the best available model of computation. That's quantum computing at the moment. This would mean that current computers can only slowly emulate quantum computing that might be essential for open-ended evolution. And this also leads to the question whether we really need continuous (uncountable) ontology of the quantum mechanics to get quantum computer behaviour: [Is bounded-error quantum polynomial time (BQP) class can be polynomially solved on machine with discrete ontology?](https://www.reddit.com/r/DigitalPhilosophy/comments/9lyeft/is_boundederror_quantum_polynomial_time_bqp_class/) (countable ontology).

This area is out of my expertise so I should first understand quantum computing from the mathematical point of view: [PHYS771 Lecture 9: Quantum (by Scott Aaronson)](https://www.scottaaronson.com/democritus/lec9.html). As far as I heard that's the best introduction view of the quantum computing.

P.S. I've just read [this post](https://www.reddit.com/r/alife/comments/mz665s/what_are_the_biggest_questions_in_alife/) in r/alife.

----

## [New research forum for open-endedness](https://old.reddit.com/r/oee/comments/ocxltw/new_research_forum_for_openendedness/)

----

## [Offtopic] Anattā (non-self) from Buddhism is a predecessor of the epistemic part of the Buddha-Darwinism

**[en.wikipedia.org/wiki/Anatta](https://en.wikipedia.org/wiki/Anatta)**

Though getting non-self in the [Buddha-Darwinism](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md) from quasi-immortality leads to drastically different morals than Buddhism ones (which doesn't use quasi-immortality).

*UPD: Well, non-self is not epistemic, but ontological in Buddhism and a conclusion from ontology in Buddha-Darwinism.*

----

## Summing up meta-ethical conclusions that can be derived from Universal Darwinism taken to extremes

Yep. That's actually another possible title for the "[Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md)" article. Aka Buddha-Darwinism on objective meaning of life separated from subjective meaning of life (Cosmogonic myth from Darwinian natural selection, Quasi-immortality, Free will, Buddhism-like illusion of the “Self”)

#### Abstract

This article sums up meta-ethical conclusions that can be derived from Universal Darwinism taken to extremes. In particular it 1) applies Universal Darwinism to evaluation of Terminal values, 2) separates objective meaning of life from subjective meaning of life using notion of Quasi-immortality. That means both moral nauralism and moral non-cognitivism are right but in different areas, 3) justifies the free will as a consequence of the Universal Darwinism, 4) comes to the conclusion of Buddhism-like illusion of the “Self” as a consequence of the Quasi-immortality, 5) as a bonus gives Universal Darwinism a hypothetical and vivid Cosmogonic myth from Darwinian natural selection. The article forms a coherent system of views, which can be called Buddha-Darwinism.

----

## My comment on "Practically-A-Book Review: Yudkowsky Contra Ngo On Agents" by Scott Alexander

https://astralcodexten.substack.com/p/practically-a-book-review-yudkowsky/comment/4567560

From the end of the Part 3:

> If the malevolent agent would get more reward than the normal well-functioning tool (which we’re assuming is true; it can do various kinds of illicit reward hacking), then applying enough gradient descent to it could accidentally complete the circuit and tell it to use its agent model.

But what does this even mean? Why is malevolence important? If "dreaming" of being a real agent (using some subsystem) would output a better results for an "oracle-tool" then its loss funtion would converge on always dreaming like a real agent. There is a risk but it's not malevolent =)

And then we can imaging it dreaming of a solution to a task that is most likely to succeed if it obtains real agency and gains direct control on the sutuation. And it "knows" that for this plan to succeed it should hide it from humans.

So this turned into "lies alignment" problem. In this case why even bother with values alignment?

### Comments

By the way. What is the end-goal of humans in here? Some previous thoughts on this (very superficial and simply to start the conversation):

> Over time, human cyborgization and augmentation using AI will leave less and less human in people. In the future limit if the goal is to keep humanity in its current form, the super AI will maintain the existence of humanity as merely a ritual integrated into its goals. Just like a super AI which sole purpose is to make paper clips. In order to prevent such a dull ending ..., it is necessary that super AI come directly from digitized people (with all their values), augmented by AI. But maybe I'm overly pessimistic, and a combination of super AI with genetically modified people who are in charge and make decisions will also work.

From [Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md#darwinian-interpretation-of-the-buddhist-illusion-of-self-death-is-bad-but-the-death-of-what)

----

## [Witten Goes Anthropic (by Peter Woit)](https://www.math.columbia.edu/~woit/wordpress/?p=12604)

----

## [Support me on Patreon](https://www.patreon.com/peotrzagubisalo)

----

## Modern sciense ontology is a Last Thursdayism implicitly

#### (this doesn't diminish physics predictive power)

Especially multiverse paired with anthropic principle suffers from this. It happens because of the lack of solid [novelty emergence mechanics](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/novelty.md). Attempts to fix it give us ad hock patches to not get [Boltzmann brain](https://en.wikipedia.org/wiki/Boltzmann_brain) variant as the most probable sentient life.

----

## Novelty emergence mechanics as a core idea of any viable ontology of the universe

I'm sure that any ontology that desires to be applicable to the universe as a whole should contain novelty emergence mechanics.

> Before natural selection was discovered it was natural to believe-assume that the entire universe was created by primordial general intelligence (aka God) as intelligence was the only known thing capable of explaining novelty emergence. Evolution and natural selection is the best explanation for novelty emergence that we have at the moment: an endless process of survival and accumulation of novelty.

*Quote from [Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md)* aka *Buddha-Darwinism on objective meaning of life separated from subjective meaning of life (Cosmogonic myth from Darwinian natural selection, Quasi-immortality, Free will, Buddhism-like illusion of the “Self”)*.

Desire for novelty emergence explanation comes from reformulated ancient question "why is there something rather
than nothing?". Reformulated into: "why these structures exist instead of other?"

And at the moment we really don't have a better mechanism-explanation for novelty emergence (in general) than natural selection.

Hence it would be a good try to embrace [Universal Darwinism](https://m.wikipedia.org/wiki/Universal_Darwinism) as an important part of a hypothetical ontology suitable for the universe as a whole. But surely natural selection by itself is not enough for ontology. But I believe that it's one of the core components.

### [Comments](../comments/reddit_digitalphilosophy_2.pdf)

----

## Evaluating terminal values

How to evaluate terminal values of humans (defined like on [lesswrong](https://www.lesswrong.com/tag/terminal-value))? Quote:

> A terminal value (also known as an intrinsic value) is an ultimate goal, an end-in-itself. ... In an [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) with a [utility](https://www.lesswrong.com/tag/utility-functions) or reward function, the terminal value is the maximization of that function.

Values are subjective but the question asks for some objective perspective. This question is of interest as “Humans' terminal values are often mutually contradictory, inconsistent, and changeable”.

Obviousness of natural selection (NS) can pose some constraints, albeit weak ones, as all known systems with sentient agents abide NS. But weak constraints are still better than no constraints at all.

Terminal goals are being split by natural selection into ones that fail to reproduce / maintain themselves and ones that survive (together with their bearers of cource). And sometimes we can even predict whether some terminal goals would go extinct or at least range their probability of survival (we already had put aside instrumental goals that “die” when they lose their purpose.).

So that's it. That's the only way to objectively judge terminal values I'm aware of. And judgment part comes from a feeling that I don't want to be invested in terminal goals that would most likely go extinct. At least they should be “mutated” in way to balance minimization of their change and maximization of their survival probability to be appealing.

*Are you aware of any other ways to evaluate terminal values?*

P.S. Basically, that post was a recap of a part of the more poetic and “old school” article that I've written: [Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md). The article doesn't add anything important to the question of this post but mostly stretches Universal Darwinism in other direstions instead.

----

## Buddha-Darwinism on objective meaning of life separated from subjective meaning of life (Cosmogonic myth from Darwinian natural selection, Quasi-immortality, Free will, Buddhism-like illusion of “Self”)

#### aka Applying Universal Darwinism to evaluation of Terminal values gives “Buddarwinism”

----

## Do you see a way use abstract rewriting system over graphs from Wolfram Physics Project to represent code-data dual algothms that modify each other and form natural selection process?

Article [Some Relativistic and Gravitational Properties of the Wolfram
Model (by Jonathan Gorard - 2020)](https://www.wolframcloud.com/obj/wolframphysics/Documents/some-relativistic-and-gravitational-properties-of-the-wolfram-model.pdf) has interesting definition of abstract rewriting systems that work on graphs.

Do you see a way to use them to represent [code-data dual](https://en.wikipedia.org/wiki/Code_as_data) algorithms that modify each other and form natural selection process? That could be a nice base for open-ended natural selection a-life model.

More info:

* [Open-ended natural selection of interacting code-data-dual algorithms as a property analogous to Turing completeness](https://www.reddit.com/r/DigitalPhilosophy/comments/dzghec/openended_natural_selection_of_interacting/)
* [Wolfram Physics Project](https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/?source=frontpage-latest-news)

### [Comments](../comments/reddit_compsci_2.pdf)

----

## [Path to the Fundamental Theory of Physics by Stephen Wolfram (Wolfram Physics Project)](https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful)

### Comments

That's an interesting direction of research. Can it actually formaulate QFT in other postulates that are more computer science friendly? If not then how to bridge the enormous gap between simple rules + complex graph dynamics vs. QFT?

Possible workaround is to get a probabilistic model from simulation graph dynamics then run this new model as a simulation. But that would require emergence of "something" to abstact it in the new model. And if we are ever to automate this abstracting the fundamental-basic model and abstracted should share fractal structure (whatever that means).

In my opinion this way would better work with ontology of indeternimistic natural selection process that branches. Like [Open-ended natural selection of interacting code-data-dual algorithms as a property analogous to Turing completeness](https://www.reddit.com/r/DigitalPhilosophy/comments/dzghec/openended_natural_selection_of_interacting/) (and [more details with a discussion on r/compsci](https://www.reddit.com/r/compsci/comments/97s8dl/on_natural_selection_of_the_laws_of_nature/)).

So probably Wolfram Physics Project lacks indeterminism.

----

## The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities

https://arxiv.org/abs/1803.03453

Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.

Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J. Bentley, Samuel Bernard, Guillaume Beslon, David M. Bryson, Patryk Chrabaszcz, Nick Cheney, Antoine Cully, Stephane Doncieux, Fred C. Dyer, Kai Olav Ellefsen, Robert Feldt, Stephan Fischer, Stephanie Forrest, Antoine Frénoy, Christian Gagné, Leni Le Goff, Laura M. Grabowski, Babak Hodjat, Frank Hutter, Laurent Keller, Carole Knibbe, Peter Krcah, Richard E. Lenski, Hod Lipson, Robert MacCurdy, Carlos Maestre, Risto Miikkulainen, Sara Mitri, David E. Moriarty, Jean-Baptiste Mouret, Anh Nguyen, Charles Ofria, Marc Parizeau, David Parsons, Robert T. Pennock, William F. Punch, Thomas S. Ray, Marc Schoenauer, Eric Shulte, Karl Sims, Kenneth O. Stanley, François Taddei, Danesh Tarapore, Simon Thibault, Westley Weimer, Richard Watson, Jason Yosinski

----

## Computational complexity as an ultimate constraint on evolution

Artem Kaznatcheev

https://www.biorxiv.org/content/10.1101/187682v4

doi: https://doi.org/10.1101/187682

Abstract

Experiments show that evolutionary fitness landscapes can have a rich combinatorial structure due to epistasis. For some landscapes, this structure can produce a computational constraint that prevents evolution from finding local fitness optima -- thus overturning the traditional assumption that local fitness peaks can always be reached quickly if no other evolutionary forces challenge natural selection. Here, I introduce a distinction between easy landscapes of traditional theory where local fitness peaks can be found in a moderate number of steps and hard landscapes where finding local optima requires an infeasible amount of time. Hard examples exist even among landscapes with no reciprocal sign epistasis; on these semi-smooth fitness landscapes, strong selection weak mutation dynamics cannot find the unique peak in polynomial time. More generally, on hard rugged fitness landscapes that include reciprocal sign epistasis, no evolutionary dynamics -- even ones that do not follow adaptive paths -- can find a local fitness optimum quickly. Moreover, on hard landscapes, the fitness advantage of nearby mutants cannot drop off exponentially fast but has to follow a power-law that long term evolution experiments have associated with unbounded growth in fitness. Thus, the constraint of computational complexity enables open-ended evolution on finite landscapes. Knowing this constraint allows us to use the tools of theoretical computer science and combinatorial optimization to characterize the fitness landscapes that we expect to see in nature. I present candidates for hard landscapes at scales from single genes, to microbes, to complex organisms with costly learning (Baldwin effect) or maintained cooperation (Hankshaw effect). Just how ubiquitous hard landscapes (and the corresponding ultimate constraint on evolution) are in nature becomes an open empirical question.

### Comments

> Check out Donald Hoffman: https://www.youtube.com/watch?v=oYp5XuGYqqY&t=2s
>
> Great at explaining this stuff.

([u/eelson99](https://old.reddit.com/r/DigitalPhilosophy/comments/e1sk5r/computational_complexity_as_an_ultimate/frljvui/))

----

## [Introduction to Artificial Life for People who Like AI](https://thegradient.pub/an-introduction-to-artificial-life-for-people-who-like-ai/)

----

## [How the “bigger is better” mentality damages AI research](https://bdtechtalks.com/2019/11/25/ai-research-neural-networks-compute-costs/)

----

## Universal Darwinism as a process of Bayesian inference

John O. Campbell

https://arxiv.org/abs/1606.07937

(Submitted on 25 Jun 2016)

Many of the mathematical frameworks describing natural selection are equivalent to Bayes Theorem, also known as Bayesian updating. By definition, a process of Bayesian Inference is one which involves a Bayesian update, so we may conclude that these frameworks describe natural selection as a process of Bayesian inference. Thus natural selection serves as a counter example to a widely-held interpretation that restricts Bayesian Inference to human mental processes (including the endeavors of statisticians). As Bayesian inference can always be cast in terms of (variational) free energy minimization, natural selection can be viewed as comprising two components: a generative model of an "experiment" in the external world environment, and the results of that "experiment" or the "surprise" entailed by predicted and actual outcomes of the "experiment". Minimization of free energy implies that the implicit measure of "surprise" experienced serves to update the generative model in a Bayesian manner. This description closely accords with the mechanisms of generalized Darwinian process proposed both by Dawkins, in terms of replicators and vehicles, and Campbell, in terms of inferential systems. Bayesian inference is an algorithm for the accumulation of evidence-based knowledge. This algorithm is now seen to operate over a wide range of evolutionary processes, including natural selection, the evolution of mental models and cultural evolutionary processes, notably including science itself. The variational principle of free energy minimization may thus serve as a unifying mathematical framework for universal Darwinism, the study of evolutionary processes operating throughout nature.

----

## [The crisis in physics is not only about physics (by Sabine Hossenfelder)](http://backreaction.blogspot.com/2019/10/the-crisis-in-physics-is-not-only-about.html)

----

## Metaphysics is dead, long live the Applied Metaphysics! (on closing philosophical questions)

This article closes philosophical questions that bothered me for quite a long time. All what is left are science questions.

After writing [Are Universal Darwinism and Occam's razor enough to answer all Why? (Because of what?) questions?](https://www.reddit.com/r/DigitalPhilosophy/comments/9kdmll/are_universal_darwinism_and_occams_razor_enough/) article I finally understood what's the place of the Metaphysics in the modern Science.

Ancient metaphysical question **"Why is there something rather than nothing?"** is obviously answered **"It just is"** and obviously is reformulated into **"Why these structures exist instead of other structures?"**. I suppose the second question should be delegated to Science that should create a mathematical model of the Universe that is capable of answering all such questions. Our Universe should be **possible** in that model and existence of sentient life should be **probable** in such model. The model should be capable of giving predictions of the future (at it should be the very same model that gave explanations - not some ad hock addition). Let's call such a theory The Ultimate Theory (TUT) (like Douglas Adams's "The Ultimate Question of Life, the Universe, and Everything").

Mainstream physics is not eager to create such a theory and is just happy about [Grand Unified Theories](https://en.wikipedia.org/wiki/Grand_Unified_Theory) (GUT). For some reason they also call such theories [Theories of everything](https://en.wikipedia.org/wiki/Theory_of_everything) (ToE). But I fail to see how they are significantly different. There are theories from non-mainstream physics that are commonly also called Theories of Everything. As far as I know such theories are not capable of answering all such questions.

But what is the philosophical justification for The Ultimate Theory? How can it even claim to answer all **"Why these structures exist instead of other structures?"** questions? The answer is simple and as obvious as it can be. Let's assume that we have a theory that can answer all questions about reality. Such answers would either be postulates of the model or conclusions from the postulates. Conclusions part is obvious - that's exactly the meaning of "answering". But what about postulates? Why are they the way they are? And the obvious answer is **"They just are"** - we should start from something after all. If the theory is capable of answering all those questions then it's enough. That's our best idea about TUT. What if there would be another TUT? The one in which out Universe is more probable is better (assuming that they are equal in other aforementioned regards). If we would have several theories with equal probability of our Universe then they would constitute an equivalence class. And the objective part is abstracted this way. Like the notion of computability is abstracted in Turing completeness property or Gauge invariance to some constant ("Gauging away" as Lee Smolin called it).

So the two key ideas that close philosophical questions are:

* "They just are"
* Abstracting into single equivalence class all differences that are left

So what is left for metaphysics then? The good example of using metaphysical considerations in aid of creating of ToE is a [Temporal naturalism](http://arxiv.org/abs/1310.8539) article by Lee Smolin. There metaphysics ideas are used for creating a scientific theory (applied!).

Metaphysics is dead, long live the Applied Metaphysics!

*previous posts on topic are in [digital philosophy subreddit](https://www.reddit.com/r/DigitalPhilosophy/) (posts by kiwi0fruit)*

### Comments

#### 1.0

A bit [about applied metaphysics in Wikipedia](https://en.wikipedia.org/wiki/Metaphysics#Applied_metaphysics)

#### 2.0

> > Ancient metaphysical question **"Why is there something rather than nothing?"** is obviously answered **"It just is"** and obviously is reformulated into **"Why these structures exist instead of other structures?"**.
>
>These aren't good questions, or should I say, they aren't going to ever be answered.
>
>We can ask how *what we do have* came to be, but asking *what else could have been* is either extremely speculative or just nonsensical.
>
>I feel like there is a razor in here somewhere...... *"You cannot derive a **could have been** from an **is**." - The Mogget*
>
>There is only one reality we have access and it only can be what it can be, and we have no way of even speculating on what other realities or other options for our reality might be.

([ThMogget@Reddit](https://www.reddit.com/r/PhilosophyofScience/comments/e03ctn/comment/f8bxnnn))

#### 2.1

I do not agree but I find it plausible that lack of knowledge of what could have been would make future predicting models disjoint with explanation models of the past that operate on probabilities of events.

But may be there is still some hope for them not to be disjoint. Reminds me of [this](https://www.reddit.com/r/PhilosophyofScience/comments/dc6k9y/thus_one_possible_way_that_the_finetuning_problem/)

----

## What’s holding artificial life back from open-ended evolution?

Emily Dolson, Anya Vostinar, Charles Ofria.

[thewinnower.com/papers/2309](https://thewinnower.com/papers/2309-what-s-holding-artificial-life-back-from-open-ended-evolution)

Evolutionary artificial life systems have demonstrated many exciting behaviors. However, there is a general consensus that these systems are missing some element of the consistent evolutionary innovation that we see in nature. Many have sought to create more "open-ended" evolutionary systems in which no stagnation occurs, but have been stymied by the difficulty of quantifying progress towards such a nebulous concept. Here, we propose an alternate framework for thinking about these problems. By measuring obstacles to continued innovation, we can move towards a mechanistic understanding of what drives various evolutionary dynamics. We propose that this framework will allow for more rigorous hypothesis testing and clearer applications of these concepts to evolutionary computation.

----

## Open-ended natural selection of interacting code-data-dual algorithms as a property analogous to Turing completeness [this time no redundant info]

#### (also on Novel stable complexity emegrence)

The goal of this article is to promote an unsolved mathematical modelling problem (not a math problem or question). And unlike math questions it still doesn't have a formal definition. But I still find it clear enough and quite interesting. I came to this modelling problem from a philosophy direction but the problem is interesting in itself.

#### Preamble

The notion of Turing completeness is a formalization of computability and algorithms (that previously were performed by humans and DNA). There are different formalizations (incl. Turing machine, μ-recursive functions and λ-calculus) but they all share the Turing completeness property and can perform equivalent algorithms. Thus they form an equivalence class.

The open-ended evolution (OEE) is a not very popular research program which goal is to build an artificial life model with natural selection which evolution doesn't stop on some level of complexity but can progress further (ultimately to the intelligent agents after some enormous simulation time). I'm not aware of the state of the progress of open-endedness criteria formulation but I'm almost sure that it's still doesn't exist: as it's either connected to results of a successful simulation or to actually understanding and confirming what is required for open-endedness (I haven't heard of either).


#### The modelling problem

Just as algorithms performed by humans were formalized and property of Turing completeness was defined: the same formalization presumably can be done to the open-ended evolution observed in nature. It went from precellular organisms to unicellular organisms and finally to Homo sapiens driven by natural selection postulates (reproduction-doubling, heredity, variation-random, selection-death, individuals-and-environment/individuals-are-environment). The Red Queen hypothesis and cooperation-competition balance resulted in increasing complexity. Open-endedness property here is analogous to Turing completeness property. It could be formalized differently but it still would form an equivalence class.

And the concise formulation of this process would be something like **Open-ended natural selection of interacting code-data-dual algorithms**.

Code-data duality is needed for algorithms being able to modify each other or even themselves. I can guess that open-endedness may incorporate some weaker "future potency" form of Turing completeness (if to assume discrete ontology with finite space and countable-infinite time then algorithms can became arbitrary complex and access infinite memory only in infinity time limit).

Please consider if it's an interesting mathematical modelling problem for research and share your thoughts.


#### Appendix: My contribution to open-ended evolution research program

My contribution to open-ended evolution research program comes from philosophy direction. The minimal model with *Open-ended natural selection of interacting code-data-dual algorithms* (or an equivalence class of minimal models) is a quite good canditate for a model of the Universe on the deepest level - as models with OEE are models of *novel stable complexity emegrence* (NSCE). Desire for NSCE explanation comes from reformulated ancient question “why is there something rather than nothing?”. Reformulated into: “why these structures exist instead of other?” And at the moment we really don't have a better mechanism-explanation for NSCE (in general) than natural selection. It should not only emerge but stay in a stable state too. It's intuitive that we can investigate very simple models for being suitable to contain OEE - as it's philosophically intuitive for a deepest level of the Universe to be relatively simple with even space dimensions and a big part of the laws of nature being emergent (formed as a result of [natural selection for a very long time](https://en.wikipedia.org/wiki/Cosmological_natural_selection)). We can even assume beginning of the Universe from a very simple (may be even “singular”) state that with time became more complex via dynamic with Natural Selection postulates: reproduction, heredity, variation aka random, selection aka death, individuals and (are) environment. Novelty and complication of structure comes from random-variation influensing heredity laws (code-data-dual algorithms reproducing and partially randomly modifying each other). Hence simple and ontologically basic models seem to be promising investigation direction for OEE research program (and may make it easier to solve).


#### Appendix: Novel stable complexity emegrence

Worth noting that it's also important to explore other ways the novel stable complexity can emerge. Before natural selection was discovered it was natural to believe-assume that the entire universe was created by primordial general intelligence (aka God) as intelligent design was the only known thing capable of NSCE (albeit being a far from ideal explanation). Evolution and natural selection (NS) is the best explanation for NSCE that we have at the moment: an endless process of survival and accumulation of novelty. But it's possible that there are other way of novelty emergence that are better than NS. So it's worth be open and keep abreast.


#### Appendix: Possible open-ended evolution research directions (self-reference, quantum computers, discrete ontology might not be enough)

* [Self-referential basis of undecidable dynamics: from The Liar Paradox and The Halting Problem to The Edge of Chaos](https://arxiv.org/abs/1711.02456),
* The discrete ontology might not be enough to express our current universe. See [discussion](https://www.reddit.com/r/math/comments/9m2ic0/is_boundederror_quantum_polynomial_time_bqp_class/) for “*Is bounded-error quantum polynomial time (BQP) class can be polynomially solved on machine with discrete ontology?*”:
  > What is your opinion and thoughts about possible ways to get an answer whether problems that are solvable on quantum computer within polynomial time ([BQP](https://en.wikipedia.org/wiki/BQP)) can be solved withing polynomial time on hypothetical machine that has discrete ontology? The latter means that it doesn't use continuous manifolds and such. It only uses discrete entities and maybe rational numbers as in discrete probability theory? By discrete I meant countable.


#### Further info links

* [article on my (futile) efforts to solve this problem](https://github.com/kiwi0fruit/ultimate-question/README.md) and it's [old Reddit discussion](https://www.reddit.com/r/compsci/comments/97s8dl/on_natural_selection_of_the_laws_of_nature/) (lots of comments)
* UPD: More in-details description of this same way of thinking is given in this section of the *[Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md#cosmogonic-myth-from-darwinian-natural-selection-in-details)* article.
* [open-ended evolution subreddit](https://www.reddit.com/r/oee/)
* [r/DigitalPhilosophy](https://www.reddit.com/r/DigitalPhilosophy/) subreddit (posts by kiwi0fruit)

### [Comments](../comments/reddit_digitalphilosophy.pdf)

----

## Whispers From the Chess Community (crosspost from r/artificial about Alpha Zero chess)

*crosspost from [r/artificial](https://www.reddit.com/r/artificial/comments/7miec4/whispers_from_the_chess_community/)*

I'm new here, and don't have the technical expertise of others in this subreddit. Nonetheless, I'm posting here to let folks here know about the whispers going around in the chess community.

I'm a master level chess player. Many of my master colleagues are absolutely stunned by the Alpha Zero games that were just released. I know this won't be new ground for many here, but for context, computers (until now) can't actually play chess. Programmers created algorithms based on human input, that allowed computers to turn chess into a math problem, then calculate very deeply for the highest value. This allowed the creation of programs that played at around the rating level 3200, compared to roughly 2800 for the human world champion. However, computers haven't really advanced much in the last five years, because it's very difficult for them to see deeper. Each further move deeper makes the math (move tree) exponentially larger, of course.

So you've probably heard that Alpha Zero learned to play chess in four hours, and then crushed the strongest computer on the market. None of that is a surprise.

However, what is truly remarkable is the games themselves. You can't really fathom it unless you play chess at a high level, but they are **very human**, and unlike anything the chess world has ever seen. They are clearly the strongest games ever played, and are almost works of art. Alpha Zero does things that are unthinkable, like playing **very long-term** positional sacrifices, things that until now have really only been accomplished by a handful of the best human players to ever live, like Anatoly Karpov. This would be like Alpha Zero composing a poem, or creating a Master level painting.

Some chess masters have even become suspicious, and believe Google must already have strong AI that it hasn't publicly acknowledged. One master friend asserted this conspiracy theory outright. Another (who happens to be a world expert in nanotechnology) estimated that the odds of Google secretly possessing strong AI is 20%, based on these games.

I would love your thoughts on this.

*[discussion](https://www.reddit.com/r/artificial/comments/7miec4/whispers_from_the_chess_community/)*

----

## Open-endedness as Turing completeness analogue for population of self organizing algorithms

#### Open-ended natural selection of interacting code-data-dual algorithms as a property analogous to Turing completeness

The goal of this article is to promote an unsolved mathematical modelling problem (not a math problem or question). And unlike math questions it still doesn't have a formal definition. But I still find it clear enough and quite interesting. I came to this modelling problem from a philosophy direction but the problem is interesting in itself.

#### Preamble

The notion of Turing completeness is a formalization of computability and algorithms (that previously were performed by humans and DNA). There are different formalizations (incl. Turing machine, μ-recursive functions and λ-calculus) but they all share the Turing completeness property and can perform equivalent algorithms. Thus they form an equivalence class.

The open-ended evolution is a not very popular research program which goal is to build an artificial life model with natural selection which evolution doesn't stop on some level of complexity but can progress further (ultimately to the intelligent agents after some enormous simulation time). I'm not aware of the state of the progress of open-endedness criteria formulation but I'm almost sure that it's still doesn't exist: as it's either connected to results of a successful simulation or to actually understanding and confirming what is required for open-endedness (I haven't heard of either).


#### The modelling problem

Just as algorithms performed by humans were formalized and property of Turing completeness was defined: the same formalization presumably can be done to the open-ended evolution observed in nature. It went from precellular organisms to unicellular organisms and finally to Homo sapiens driven by natural selection postulates (reproduction-doubling, heredity, variation-random, selection-death, individuals-and-environment/individuals-are-environment) and the Red Queen hypothesis that resulted in increasing complexity. Open-endedness property here is analogous to Turing completeness property. It could be formalized differently but it still would form an equivalence class.

And the concise formulation of this process would be something like **Open-ended natural selection of interacting code-data-dual algorithms**.

Code-data duality is needed for algorithms being able to modify each other or even themselves. I can guess that open-endedness may incorporate some weaker "future potency" form of Turing completeness (if to assume discrete ontology with finite space and countable-infinite time then algorithms can became arbitrary complex and access infinite memory only in infinity time limit).

Please consider if it's an interesting mathematical modelling problem for research and share your thoughts.


#### Further info links

* [open-ended evolution subreddit](https://www.reddit.com/r/oee/)
* [article on my (futile) efforts](https://github.com/kiwi0fruit/ultimate-question) and it's [old Reddit discussion](https://www.reddit.com/r/compsci/comments/97s8dl/on_natural_selection_of_the_laws_of_nature/)
* [digital philosophy subreddit](https://www.reddit.com/r/DigitalPhilosophy/) (posts by kiwi0fruit)

*Below is a predecessor of this promotion article:*

#### Open-endedness as Turing completeness analogue for population of self organizing algorithms

Recently I wrote small article named "Simplest open-ended evolution model as a theory of everything". But right after finishing it I noticed that theory of everything part was just a guide and crutch to a more interesting point of view.

Specifically that property of open-endedness (that is yet to be discovered) can be viewed as Turing completeness analogue for population of self organizing algorithms under natural selection (where each program is also data). And my research program was essentially about finding **necessary and sufficient** criteria for open ended evolution (OEE). Plus may be some intuitions about directions in which it can be found (most notable is applying simplest OEE model to the beginning of the artificial universe). Hence all philosophical questions that bothered me are now reduced to necessary and sufficient criteria for open ended evolution that is no longer a philosophical question at all (for philosophical part see [this acticle](https://www.reddit.com/r/DigitalPhilosophy/comments/9kdmll/are_universal_darwinism_and_occams_razor_enough/)).

#### UPD

If turing completeness is a formalization of algorithms (that previously were performed by humans only). I'm interested in formalization of natural selection open-endedness that is now observed in nature (called OEE). That's what my post is about essentially. That formalization is still not there. It's an open and a hard question.

*Text of the original article*:


#### Simplest open-ended evolution model as a theory of everything

Year ago I abandoned the research project ([old Reddit discussion](https://www.reddit.com/r/compsci/comments/97s8dl/on_natural_selection_of_the_laws_of_nature/), [article](https://github.com/kiwi0fruit/ultimate-question), [subscribe on Reddit](https://www.reddit.com/r/DigitalPhilosophy)). But from now on I hope to spend on it at least a few hours per week. To start with let's remember cornerstones of this research program:


#### 1. Open-ended evolution

**Open-ended evolution** (OEE) model:

* contains **natural selection** (NS) postulates (reproduction-doubling, heredity, variation-random, selection-death, individuals-**and**-environment/individuals-**are**-environment).
* in which the evolution doesn't stop on some level of complexity but can progress further to the intelligent agents after some great time.
* that should presumably incorporate: **turing-completeness** (or it's weaker "future potency" form) and **Red Queen hypothesis**.


#### 2. Theory of everything

By **Theory of everything** I mean:

* dynamic model of an artificial universe in which after some enormous simulation time properties of our universe is **possible** (but not necessary highly probable) but existing of intelligent life is highly **probable**.
* model that is capable of answering **all** in-model "**why these** structures exist and processes take place instead of the other?" questions by combination of transition rules postulates application and history of events (including completely random events).
* it may be desirable to have a **universal description tool** that can be applied to any "level" of the model (where "higher" levels are built upon many smaller modules. But the picture would be more complicated if strange loops are possible). Level hierarchy can be alike to organelles -> cells -> species individuals -> packs/tribes -> populations.


#### 3. Simplest

By **simplest** I mean:

* As less axioms that govern evolution of the model as possible: **Occam's razor** (OR) plus extracting **necessary and sufficient** (NaS) system transition rules that still give OEE (it may even be some equivalence class property like turing-completeness).
* In the model time is **discrete** and **countable infinite** (given by random events), there was the **first moment** of existence, space is **discrete** and **finite**. We can try starting thinking about it with a graph-like structure with individuals of NS as nodes - graph is the simplest space possible.
* This raises question: What about quantum computers? Is bounded-error quantum polynomial time (BQP) class can be polynomially solved on machine with **discrete ontology**? And if yes what should this ontology be?
* Also I guess some may argue for lack of random events and going Everett many world quantum mechanics (QM) interpretation way. Can model
be viewed as a "superposition" of random events happened in different universes? If yes then we may get uncountable infinite space-time (btw: would superposition in QM preserve countable infinity for space-time?).


#### 4. UPD

I dropped seriously investing in my research not long before I discovered connections with OEE and even then I wasn't aware that the only notable part of my research is OEE question part (hence I simply reinvented the ~~wheel~~ question but moved from philosophy side). Since publication of this post I'm aware of that so investing in finding out what is open-endedness is inevitable if I want to progress on this task.

----

## Defining and simulating open-ended novelty: requirements, guidelines, and challenges

2016 http://doursat.free.fr/docs/Banzhaf_et_al_2016_OEE_TheoBiosci.pdf

Wolfgang Banzhaf, Bert Baumgaertner, Guillaume Beslon, René Doursat, James A. Foster, Barry McMullin, Vinicius Veloso de Melo, Thomas Miconi, Lee Spector, Susan Stepney, Roger White

The open-endedness of a system is often defined as a continual production of novelty. Here we pin down this concept more fully by defining several types of novelty that a system may exhibit, classified as variation, innovation, and emergence. We then provide a meta-model for including *levels of structure* in a system’s model. From there, we define an architecture suitable for building simulations of open-ended novelty-generating systems and discuss how previously proposed systems fit into this framework. We discuss the design principles applicable to those systems and close with some challenges for the community.

----

## Philosophy helps to raise Questions and gives intuitions to seek Answers. But good answered question would be no longer philosophy but science. Beware of implicit philosophical assumptions you use!

----

## Self-referential basis of undecidable dynamics: from The Liar Paradox and The Halting Problem to The Edge of Chaos

https://arxiv.org/abs/1711.02456

Mikhail Prokopenko, Michael Harré, Joseph Lizier, Fabio Boschetti, Pavlos Peppas, Stuart Kauffman

(Submitted on 7 Nov 2017 (v1), last revised 21 Mar 2019 (this version, v2))

In this paper we explore several fundamental relations between formal systems, algorithms, and dynamical systems, focussing on the roles of undecidability, universality, diagonalization, and self-reference in each of these computational frameworks. Some of these interconnections are well-known, while some are clarified in this study as a result of a fine-grained comparison between recursive formal systems, Turing machines, and Cellular Automata (CAs). In particular, we elaborate on the diagonalization argument applied to distributed computation carried out by CAs, illustrating the key elements of Gödel’s proof for CAs. The comparative analysis emphasizes three factors which underlie the capacity to generate undecidable dynamics within the examined computational frameworks: (i) the program-data duality; (ii) the potential to access an infinite computational medium; and (iii) the ability to implement negation. The considered adaptations of Gödel’s proof distinguish between computational universality and undecidability, and show how the diagonalization argument exploits, on several levels, the self-referential basis of undecidability.

----

## [An Overview of Open-Ended Evolution: Editorial Introduction to the Open-Ended Evolution II Special Issue](https://arxiv.org/abs/1909.04430?)

----

## Is bounded-error quantum polynomial time (BQP) class can be polynomially solved on machine with discrete ontology?

*crosspost from [reddit.com/r/math/comments/9m2ic0](https://www.reddit.com/r/math/comments/9m2ic0/is_boundederror_quantum_polynomial_time_bqp_class/)*

What is your opinion and thoughts about possible ways to get an answer whether problems that are solvable on quantum computer within polynomial time (BQP) can be solved withing polynomial time on hypothetical machine that has discrete ontology? The latter means that it doesn't use continuous manifolds and such. It only uses discrete entities and maybe rational numbers as in discrete probability theory?

upd: by discrete I meant countable.

### [Comments](../comments/reddit_math.pdf)

----

## "God made the integers; all else is the work of man." (Leopold Kronecker) What would you add to the list?

I would add:

* graphs
* discrete random events
* something about providing Turing completeness

### Comments

#### 1

> Unary number system - that is to say, tally marks? :) Roman numerals, base 10, binary, etc.
>
> Hmm... actually I wonder if one could argue binary is "more fundamental" than unary... more useful, definitely.

([u/heyandy889](https://old.reddit.com/r/DigitalPhilosophy/comments/9kivsj/god_made_the_integers_all_else_is_the_work_of_man/eaot4xl/))

#### 2

> The list of what god made, or the list of what man made? Both are mentioned/implied, yet 'the' is used.
>
>Understanding itself can be argued to be the work of man — and yet, it is also the work of god which created it ('man' and 'god' just being labels for 'something' and 'its creator/reason', here).
>
>The viewpoint that results can easily be argued to have been caused by both — but on any particular stage of this process, what-was-before and consequence can be separated, so one may think that some things are more fundamental than others.
>
>Yet, there are as many ways to arrive at the same conclusions as there are viewpoints: viewpoints are all ultimately functionally equivalent. What is fundamental to some, is a consequence to others. One's man is another's god.
>
>Saying what is fundamental does not describe reality, it describes the history of the one describing. Integers? That is one way of viewing the world, a more mathematical one — yet there are countless others. One should not be stuck in thinking that any one pure viewpoint is the superior/ultimate one; everything can evolve, even the understanding of understanding itself.
>
>I would maybe add…
>
>"God made the world and evolution of its part; all else is its work."
>
>"God made choices and told which are better; all else, the work of animals — the understanding, the work of man."
>
>Both are great bases for viewing reality (emergent behavior and optimization — both pure), equivalent to each other, yet not the same, behaving differently in different contexts.
>
>Also, execution of plans/programs (and time that allows it) seems very important to me.
>
>And if man had to refine and re-invent understanding, which has produced everything else, can man be said to be its own god? Ultimately, it's just semantics.

([u/Antipurity](https://old.reddit.com/r/DigitalPhilosophy/comments/9kivsj/god_made_the_integers_all_else_is_the_work_of_man/edqrftl/))

----

## Are Universal Darwinism and Occam's razor enough to answer all Why? (Because of what?) questions?

*[crosspost to r/PhilosophyofScience](https://www.reddit.com/r/PhilosophyofScience/comments/9k7v2q/are_universal_darwinism_and_occams_razor_enough/)*

I'm investigating possibilities and tools for creating a model of the Universe in which all Why? (Because of what?) questions can be answered.

The current best ideas I found are:

* Natural selection to explain structures that exist (including space properties and topology) - Universal Darwinism to full extent so as much structures as possible would have a history how they emerged in the model.
* To explain rules that govern dynamics of the model with natural selection we cannot again use natural selection. We can try use clasical combination of falsifiability and Occam's razor. The falsifiability can be applied only in a limited way (as described in pt.3 of the [main article](https://www.reddit.com/r/compsci/comments/97s8dl/on_natural_selection_of_the_laws_of_nature/)) - the current understanding of nature is far from explaining space and the set of laws of nature. So testing and predictions are unavailable for the model to create.
* Luckily we can still use Occam's razor and simplicity considerations. But it can justify only when comparing models that are practically-experimentally the same. Let's assume we extracted and proved the necessary and sufficient (NaS) rules from a set of models that provide important behavior for the model ("open-endedness" means that the evolution doesn't stop on some level of complexity but can progress further to the intelligent agents after some great time). NaS means that it's the simplest rules (may be rules be extracted with accuracy up to the isomorphism - or even property like Turing completeness). So is it enough to justify/explain the rules that govern dynamics of the model? **Yes. It's enough** as there is no other choise as to assume some predefined rules that define ontology and govent evolution and natural selection. If we get lucky the objective reality can be separated from models via equivalence class up to isomorphism (similar to "gauging away" in Physics as Lee Smolin called it).
* I'm aware that within this task some things should not be justified or explained. Natural selection postulates require "variation" that need random events that are actually just **are** and do not have a cause (the flip of a coin has a reason but whether it's heads or tails doesn't have a reason). So may be the extracted necessary and sufficient rules are also do not require explanation?

Maybe I missed something and there are other approaches to this problem (creating a model of the Universe in which all Why? questions can be answered)?

### Comments

* [Comments](../comments/reddit_philosophyofscience.pdf)
* https://en.wikipedia.org/wiki/Regress_argument is on topic.

----

## [Introduction complete rewrite: The simplest artificial life model with open-ended evolution as a possible model of the universe](https://www.reddit.com/r/compsci/comments/97s8dl/on_natural_selection_of_the_laws_of_nature/)

#### On natural selection of the laws of nature, Artificial life and Open-ended evolution, Universal Darwinism, Occam's razor

#### The simplest artificial life model with open-ended evolution as a possible model of the universe, Natural selection of the laws of nature, Universal Darwinism, Occam's razor

#### NEW UPDATES

#### UPD: It is advised to start reading from [Open-ended natural selection of interacting code-data-dual algorithms as a property analogous to Turing completeness](https://www.reddit.com/r/DigitalPhilosophy/comments/dzghec/openended_natural_selection_of_interacting/) article.
#### UPD: [Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md) aka Buddha-Darwinism on objective meaning of life separated from subjective meaning of life (Cosmogonic myth from Darwinian natural selection, Quasi-immortality, Free will, Buddhism-like illusion of Self)
#### UPD: [Evaluating terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/evaluating_terminal_values.md)
#### UPD: [Novelty emergence mechanics as a core idea of any viable ontology of the universe](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/novelty.md)

Introduction contents:

* [Intro pt.1](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt1)
* [Intro pt.2: Key ideas](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt2-key-ideas)
* [Intro pt.3: Justification and best tools](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt3-justification-and-best-tools)
* [Intro pt.4: The model](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt4-the-model)
* [Intro pt.5: Obvious problems, incl. what is inanimate matter? what about quantum computers?](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt5-obvious-problems-incl-what-is-inanimate-matter-what-about-quantum-computers)
* [Intro pt.6: P.S., links and discuss](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt6-ps-discussion-subscribe-source-code-repository)
* [Appendix: contents of the previous article on the topic](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#appendix-contents)


#### Intro pt.1

Greetings,

I seek advice or any other help available regarding creating a specific mathematical model. It’s origin is at the intersection of the following areas:

* fundamental physics (an important bit),
* the theory of evolution (a lot),
* metaphysics (a lot),
* foundations of mathematics and theory of computation (should be a lot).

The problem I’m trying to solve can be described as **to create the simplest artificial life model possible with open-ended evolution** (open-endedness means that the evolution doesn't stop on some level of complexity but **can** progress further to the intelligent agents after some **great** time). There are analogues to laws of nature in this dynamic model: 1) postulates of natural selection + some unknown ontological basis, 2) pool of phenotypes of evolving populations that are relatively stable on some time periods so they can be considered "laws". This approach implies indeterminism and postulates random and spontaneous nature of some events. It is also assumed that the universe had the first moment of existence with relatively simple structure.


#### Intro pt.2: Key ideas

The key idea of this research program is to create an artificial universe in which we can answer any questions like "why is the present is this way not another?", "because of what?" (it's a better formulated ancient question "Why is there something rather than nothing?"). So any existing structures can be explained: as much entities as possible should have a history how they appeared/emerged - instead of postulating them directly. Moreover the model itself needs to have some justification (to be a candidate for model of the our real universe).

There are two main intuitions-constraints for this universe: 1) the start from the simple enough state (the beginning of time), 2) the complexity capable of producing sentient beings (after enormous simulation time of course) comes from natural selection which postulates are provided by universe model rules. The two intuitions give hope that **the model to build would be simple and obvious in retrospect** like postulates of natural selection are simple and obvious in **retrospect** (they are obvious, but until Darwin formulated them it was really hard to assume them). So there is a hope that **it's feasible task**.

**The model to build is a model of complexity generation. At later steps the complexity should be capable of intelligence and self-knowledge.** Sadly I have not moved far to this goal. I'm still in the situation of "I feel like the answer the this grand question can be obtained this particular way"


#### Intro pt.3: Justification and best tools

Those two intuitions come from the following:

The best tool I know that can historically explain why the particular structures exist is Darwin's evolution with natural selection. And the best tools to justify the model of reality are [falsifiability](https://en.wikipedia.org/wiki/Falsifiability) and [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor). First states that the theory should work and be capable of predictions. The second states that among models similar in respect to falsifiability the simplest one should be chosen.

If we are to go with natural selection as novelty generating mechanism then we should think that Lee Smolin's [Cosmological natural selection](https://en.wikipedia.org/wiki/Lee_Smolin#Cosmological_natural_selection) (CNS) hypothesis is likely to be true. And that means that our observable universe could have had a very large number of universes-ancestors. This means that it would be really hard to apply falsifiability to the model to build. In the best case when built (sic!) it could provide the basis for General relativity + Quantum mechanics unification theory (or could not...). In the worst case we only get the restriction that our universe is possible in the model. I.e the populations of individuals that **resemble** our laws of physics should be **probable** to appear and our **particular** laws of physics are definitely **possible** to appear (either it's a group dynamic or a single individual universe like in CNS).

Luckily we also have an artificial life open-ended evolution (a-life OEE) restriction and Occam's razor. OEE means that at least in itself the model must show specific dynamics. And we already can assume that the model should be as simple as possible (and if the assumed simplicity is not enough then we should make it more complex). Though simplicity by itself cannot be a justification I have a hope that selecting the simplest workings from many **working** a-life OEE models could be a justification (proof of the theorem that the selected workings should be in every a-life OEE would be even better). And I mean a justification for basic rules that govern the dynamic of the model. By the way, this way we can justify a model obtained via any other research program. So if some "Theory of everything" appears we don't need to ask "why this particular theory?". Instead we should check for other (simpler?) models that do their work as good and then reason of necessary and sufficient criteria.

More about justification: [Are Universal Darwinism and Occam's razor enough to answer all Why? (Because of what?) questions?](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/answer_all_why_questions.md)


#### Intro pt.4: The model

The research program uses the artificial life model with natural selection as a basis. This means taking inspiration in natural selection of biological life (**NS**). Also adding Occam's razor (**OR**) to the picture. In order to continue we need to precisely define what are individuals in the model (and environment if needed) and how the process of their replication and death takes place. There are some properties of the model we can assume and go with:

* The are **individuals and environment** (NS). _**Either**_: the individuals are the environment for other individuals - there is nothing except individuals (OR). At the beginning of the Universe there were only one or two individuals (OR). _**Or**_: there's environment of which individuals are built (and environment may not be governed by NS postulates).
* **Time is discrete and countable infinite**, there was the **first moment of existence of the Universe**, **space is discrete and finite** (OR). We can start thinking about it with a graph-like structure with individuals of NS as nodes - graph is the simplest space possible (OR, NS).
* **Reproduction**: individual has a potential to reproduce itself (NS). Individuals can **double** (OR).
* **Heredity**: properties of the individuals are inherited in reproduction (NS).
* **Variation**: when the individual reproduces itself, the reproduction does not occur precisely but with changes that are partly **random/spontaneous** (NS).
* **Natural selection**: the individuals that are more adapted to the environment survive more often (NS). It actually Captain Obvious says that "**survive those who survive**" (OR). If we use analogue with biological life then we can assume something like living in the stream of energy using the difference in entropy (so **stream-like behavior** can be put to the model). If there's nothing except individuals (no environment) then maybe node-like individuals can not only come to existence but also die and disappear.
* Natural selection and evolution are **open-ended**: they do not stop on a fixed level of complexity but instead progresses further. And they are capable of producing sentient individuals.
* The **Turing-completeness** is desired for the model: in theory there can emerge (be?) complex emergent individuals performing algorithms. Presumably **complex algorithms require a lot of space and time** so they are made up from many basic individuals.
* ...
* More **complex laws are emergent** from algorithms formed by surviving stable individuals that **change other individuals** (or environment if there's any).


#### Intro pt.5: Obvious problems, incl. what is inanimate matter? what about quantum computers?

1\. If we assume that complex laws are emergent from algorithms then **what about quantum computers?** question needs answering. It can be formulated as "Is bounded-error **quantum polynomial time (BQP) class can be polynomially solved on machine with discrete ontology?**"

What is your opinion and thoughts about possible ways to get an answer whether problems that are solvable on quantum computer within polynomial time (BQP) can be solved withing polynomial time on hypothetical machine that has discrete ontology? The latter means that it doesn't use continuous manifolds and such. It only uses discrete entities and maybe rational numbers as in discrete probability theory?

2\. If we go with natural selection, use biological life as inspiration and go with assumptions above then we should answer the question: **what is the inanimate matter?**

[**continue intro reading pt.5...**](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt5-obvious-problems-incl-what-is-inanimate-matter-what-about-quantum-computers)

#### Continue reading:

* [Intro pt.5: Obvious problems, incl. what is inanimate matter? what about quantum computers?](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt5-obvious-problems-incl-what-is-inanimate-matter-what-about-quantum-computers)
* [Intro pt.6: P.S., links and discuss](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#intro-pt6-ps-discussion-subscribe-source-code-repository)
* [Appendix contents](https://github.com/kiwi0fruit/ultimate-question/blob/master/README.md#appendix-contents)

--------

* GitHub repository of the article: [github.com/kiwi0fruit/ultimate-question](https://github.com/kiwi0fruit/ultimate-question)
* [Discussion on GitHub](https://github.com/kiwi0fruit/ultimate-question/issues/2)
* Subreddit to follow up: [r/DigitalPhilosophy](https://www.reddit.com/r/DigitalPhilosophy/) (I would also post to other subreddits but the posts can be deleted or downvoted as follow up is not that interesting as oroginal post)

#### NEW UPDATES

#### Follow up: [Open-ended natural selection of interacting code-data-dual algorithms as a property analogous to Turing completeness](https://www.reddit.com/r/DigitalPhilosophy/comments/dzghec/openended_natural_selection_of_interacting/)
#### UPD: [Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md)
#### UPD: [Evaluating terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/evaluating_terminal_values.md)
#### UPD: [Novelty emergence mechanics as a core idea of any viable ontology of the universe](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/novelty.md)

### [Comments](../comments/reddit_compsci.pdf)

----

## Personal story: "Why people need God's love?", Existential crisis, Ultimate Question of Life, the Universe, and Everything

*cross-post from [r/atheism/9iqkqm](https://www.reddit.com/r/atheism/comments/9iqkqm/personal_story_why_people_need_gods_love/). That's my personal story but it was also meant to be this subreddit advertisment. But for some reason it wasn't successful. Wrong title? Wrong publish time? No chances in that sub?*

--------

#### Why people need God's love?

There is a movie called *The Rabbi's Cat* (2011). I enjoyed it and there was a moment that struck me to the heart: when the rabbi and mullah danced together and laughed for some reason, but they mostly did it because they felt the God is with them and thought that they are loved by him. So this picture of always not being alone and always having a purpose was very informant for me (yep, even imaginary friends and righteous lords are still friends and righteous lords...). This showed a remarkable contrast to my state at the moment.


#### Existential crisis

By that time I had fully embraced deterministic scientific picture of our universe and that put me to existential crisis (*Wikipedia: moment at which an individual questions if their life has meaning, purpose, or value*) - this was partially because I ~married and felt that pursuing love was no longer the meaning of my life. But what was it then?

And that "deterministic scientific universe" is a harsh place, I tell you. Every moment of future is predefined by the past and the laws, you don't have free will (only illusion of it), you are as meaningful as a cog in a mechanism, and whatever you choose or do makes no **"real"** impact on anything. I find it impossible to find a meaning of life in such a universe. And the ones who claim that they found it for me is not that different from those who really believe in Santa Claus or imaginary lord.

So neither universe with imaginary lord nor "deterministic scientific universe" were a satisfactory place for me. This was a start of my journey to find a better idea of what our universe is.

The first discovery was that there is no need to think that our universe is deterministic (*Wikipedia: philosophical theory that all events are completely determined by previously existing causes*). All falsifiable and tested laws or nature are even better compatible with indeterministic universe: particularly because of quantum mechanics (*Wikipedia: it is the opposite of determinism and related to chance - not all events are completely predetermined*). This restored free will but it still was unclear where is the place for chance in our universe?

This way I at least can have my own meaning of my life - to create such a meaning. There is still a question if my actions can make any impact. But if the future is not predetermined then there is a change (no matter how small it is) to change it.


#### Ultimate Question of Life, the Universe, and Everything

_**THIS SECTION IS OUTDATED: I'M NO LONGER FOND OF SELF-JUSTIFICATION IDEA.**_

This also inevitably lead to the attempt to find or create the theory of everything. Search didn't give me a satisfactory theory. I had already known that the answer to the *Ultimate Question of Life, the Universe, and Everything* is 42 but this too was unsatisfactory. So I ended trying to create the theory myself. It turned out that this is a difficult task :)

Potential theories of everything can be **self-justifying** or not. It means that the theory is:

1. **theory of everything**: capable of answering all questions like "why these structures exist / processes take place instead of the other ones?". I.e. given all knowledge about the past they can (at least theoretically) track chains of causes back to the past to the moments where they came to existence.
2. **self-justifying**: capable of answering question "why the theory of everything works this way not another?". And answer "because it's predictions are in agreement with experiments" is not enough because there can be infinite number of such theories that differ in things we cannot test (yet? never? who knows...). So we can either wait for General relativity + Quantum mechanics unification (and see if there would be the same problem :) or we can try to answer this question via self-justifying. It relies on philosophical necessity, Occam's razor, Captain Obvious considerations and common sense.

As far as I know candidates for theory of everything that are being developed by physicists are not meant to be build self-justifying. But in the past the self-justifying cosmogonies were build. The simplest one starts from the sentient "self-justifying" God-creator. The god was at the beginning of time and he is self-justifying. It can be imagined as the Universe starts with artificial general intelligence agent with goals (better call it primordial general intelligence PGI instead of AGI). Then PGI creates everything else... There is a question "who created the God?" but it is still a non-contradictory way of thinking that the PGI was at the beginning of time and he doesn't need justification. This way the mind is the fundamental part of the universe (I don't believe this anyway).

I suggest to use similar approach to PGI but use natural selection instead of PGI. We know that biological natural selection is capable of producing sentient individuals and it's simpler than PGI from Occam's razor point of view (presumably PGI should be as complex as AGI). This assumes that the fundamental aspect of the Universe is the **life** (instead of PGI or predefined mechanical-like laws).

And natural selection requires random events for it's postulates so it's good fit to the free will.


#### Artificial life with Open-ended evolution for the simplest and self-justifying artificial universe, On natural selection of the laws of nature

My latest attempt to find the theory of everything can be described as *"The simplest artificial life model with open-ended evolution as a possible model of the universe, Natural selection of the laws of nature, Universal Darwinism, Occam's razor"* and discussed in [this post](https://www.reddit.com/r/compsci/comments/97s8dl/on_natural_selection_of_the_laws_of_nature/).

I noted that communities of both physicists and philosophers are not fond of my research idea. I've got the best feedback in Computer Science and in Artificial life communities. But still it is somewhat alien there. So I lacked subreddit so that such ideas are right fit there and found out that the research best fits to Digital Philosophy ideas that uses theory of computation and discrete ontology. So I invite you to the new [r/DigitalPhilosophy](https://www.reddit.com/r/DigitalPhilosophy/) subreddit.

----

## [New extremely fantastic speculations about "What is the inanimate matter?" in a model where life and natural selection are basic](../README.md#intro-pt5-obvious-problems-incl-what-is-inanimate-matter-what-about-quantum-computers)

### Comments

* [comments](../comments/reddit_digitalphilosophy_3_1.pdf)
* [continuation](../comments/reddit_digitalphilosophy_3_2.pdf)

----

## Formal logic is a collection of presumptions of reality modeling. The presumptions are so successful that they seem obvious.

Or they were simply hardcoded to our brains :)

(Just a random thought)

**upd** But this does't deny that logic is a helpful tool to reason/infer about invariant constraints/patterns/properties (inveriant to transforms or different contexts). \*Given we know about constraints that equivalent in cases we compare.

### [Comments](../comments/reddit_digitalphilosophy_4.pdf)

----
