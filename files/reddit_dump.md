## Toward a theory of evolution as multilevel learning

Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin

### Abstract

We apply the theory of learning to physically renormalizable systems in an attempt to outline a theory of biological evolution, including the origin of life, as multilevel learning. We formulate seven fundamental principles of evolution that appear to be necessary and sufficient to render a universe observable and show that they entail the major features of biological evolution, including replication and natural selection. It is shown that these cornerstone phenomena of biology emerge from the fundamental features of learning dynamics such as the existence of a loss function, which is minimized during learning. We then sketch the theory of evolution using the mathematical framework of neural networks, which provides for detailed analysis of evolutionary phenomena. To demonstrate the potential of the proposed theoretical framework, we derive a generalized version of the Central Dogma of molecular biology by analyzing the flow of information during learning (back propagation) and predicting (forward propagation) the environment by evolving organisms. The more complex evolutionary phenomena, such as major transitions in evolution (in particular, the origin of life), have to be analyzed in the thermodynamic limit, which is described in detail in the paper by Vanchurin et al.

https://pubmed.ncbi.nlm.nih.gov/35121666/

(free access)

----

## [Dmitry Shabanov: On the immortality of populations (autotranslated)](https://old-computerra-ru.translate.goog/own/shabanov/612801/?_x_tr_sl=ru&_x_tr_tl=en&_x_tr_hl=ru)

Original: https://old.computerra.ru/own/shabanov/612801/

----

## What are the biggest questions in ALife? My take: are quantum computing capabilities essential to get open-ended evolution?

More specificaly it's what ontology is enough to get [open-ended evolution](https://www.reddit.com/r/oee/). I have no idea... My best guess is to apply the best available model of computation. That's quantum computing at the moment. This would mean that current computers can only slowly emulate quantum computing that might be essential for open-ended evolution. And this also leads to the question whether we really need continuous (uncountable) ontology of the quantum mechanics to get quantum computer behaviour: [Is bounded-error quantum polynomial time (BQP) class can be polynomially solved on machine with discrete ontology?](https://www.reddit.com/r/DigitalPhilosophy/comments/9lyeft/is_boundederror_quantum_polynomial_time_bqp_class/) (countable ontology).

This area is out of my expertise so I should first understand quantum computing from the mathematical point of view: [PHYS771 Lecture 9: Quantum (by Scott Aaronson)](https://www.scottaaronson.com/democritus/lec9.html). As far as I heard that's the best introduction view of the quantum computing.

P.S. I've just read [this post](https://www.reddit.com/r/alife/comments/mz665s/what_are_the_biggest_questions_in_alife/) in r/alife.

----

## [New research forum for open-endedness](https://old.reddit.com/r/oee/comments/ocxltw/new_research_forum_for_openendedness/)

----

## [Offtopic] Anattā (non-self) from Buddhism is a predecessor of the epistemic part of the Buddha-Darwinism

**[en.wikipedia.org/wiki/Anatta](https://en.wikipedia.org/wiki/Anatta)**

Though getting non-self in the [Buddha-Darwinism](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md) from quasi-immortality leads to drastically different morals than Buddhism ones (which doesn't use quasi-immortality).

*UPD: Well, non-self is not epistemic, but ontological in Buddhism and a conclusion from ontology in Buddha-Darwinism.*

----

## Summing up meta-ethical conclusions that can be derived from Universal Darwinism taken to extremes

Yep. That's actually another possible title for the "[Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md)" article. Aka Buddha-Darwinism on objective meaning of life separated from subjective meaning of life (Cosmogonic myth from Darwinian natural selection, Quasi-immortality, Free will, Buddhism-like illusion of the “Self”)

### Abstract

This article sums up meta-ethical conclusions that can be derived from Universal Darwinism taken to extremes. In particular it 1) applies Universal Darwinism to evaluation of Terminal values, 2) separates objective meaning of life from subjective meaning of life using notion of Quasi-immortality. That means both moral nauralism and moral non-cognitivism are right but in different areas, 3) justifies the free will as a consequence of the Universal Darwinism, 4) comes to the conclusion of Buddhism-like illusion of the “Self” as a consequence of the Quasi-immortality, 5) as a bonus gives Universal Darwinism a hypothetical and vivid Cosmogonic myth from Darwinian natural selection. The article forms a coherent system of views, which can be called Buddha-Darwinism.

----

## My comment on "Practically-A-Book Review: Yudkowsky Contra Ngo On Agents" by Scott Alexander

https://astralcodexten.substack.com/p/practically-a-book-review-yudkowsky/comment/4567560

From the end of the Part 3:

> If the malevolent agent would get more reward than the normal well-functioning tool (which we’re assuming is true; it can do various kinds of illicit reward hacking), then applying enough gradient descent to it could accidentally complete the circuit and tell it to use its agent model.

But what does this even mean? Why is malevolence important? If "dreaming" of being a real agent (using some subsystem) would output a better results for an "oracle-tool" then its loss funtion would converge on always dreaming like a real agent. There is a risk but it's not malevolent =)

And then we can imaging it dreaming of a solution to a task that is most likely to succeed if it obtains real agency and gains direct control on the sutuation. And it "knows" that for this plan to succeed it should hide it from humans.

So this turned into "lies alignment" problem. In this case why even bother with values alignment?

### Comments

By the way. What is the end-goal of humans in here? Some previous thoughts on this (very superficial and simply to start the conversation):

> Over time, human cyborgization and augmentation using AI will leave less and less human in people. In the future limit if the goal is to keep humanity in its current form, the super AI will maintain the existence of humanity as merely a ritual integrated into its goals. Just like a super AI which sole purpose is to make paper clips. In order to prevent such a dull ending ..., it is necessary that super AI come directly from digitized people (with all their values), augmented by AI. But maybe I'm overly pessimistic, and a combination of super AI with genetically modified people who are in charge and make decisions will also work.

From [Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md#darwinian-interpretation-of-the-buddhist-illusion-of-self-death-is-bad-but-the-death-of-what)

----

## [Witten Goes Anthropic (by Peter Woit)](https://www.math.columbia.edu/~woit/wordpress/?p=12604)

----

## [Support me on Patreon](https://www.patreon.com/peotrzagubisalo)

----

## Modern sciense ontology is a Last Thursdayism implicitly

### (this doesn't diminish physics predictive power)

Especially multiverse paired with anthropic principle suffers from this. It happens because of the lack of solid [novelty emergence mechanics](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/novelty.md). Attempts to fix it give us ad hock patches to not get [Boltzmann brain](https://en.wikipedia.org/wiki/Boltzmann_brain) variant as the most probable sentient life.

----

## Novelty emergence mechanics as a core idea of any viable ontology of the universe

I'm sure that any ontology that desires to be applicable to the universe as a whole should contain novelty emergence mechanics.

> Before natural selection was discovered it was natural to believe-assume that the entire universe was created by primordial general intelligence (aka God) as intelligence was the only known thing capable of explaining novelty emergence. Evolution and natural selection is the best explanation for novelty emergence that we have at the moment: an endless process of survival and accumulation of novelty.

*Quote from [Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md)* aka *Buddha-Darwinism on objective meaning of life separated from subjective meaning of life (Cosmogonic myth from Darwinian natural selection, Quasi-immortality, Free will, Buddhism-like illusion of the “Self”)*.

Desire for novelty emergence explanation comes from reformulated ancient question "why is there something rather
than nothing?". Reformulated into: "why these structures exist instead of other?"

And at the moment we really don't have a better mechanism-explanation for novelty emergence (in general) than natural selection.

Hence it would be a good try to embrace [Universal Darwinism](https://m.wikipedia.org/wiki/Universal_Darwinism) as an important part of a hypothetical ontology suitable for the universe as a whole. But surely natural selection by itself is not enough for ontology. But I believe that it's one of the core components.

### [Comments](../comments/reddit_digitalphilosophy_2.pdf)

----

## Evaluating terminal values

How to evaluate terminal values of humans (defined like on [lesswrong](https://www.lesswrong.com/tag/terminal-value))? Quote:

> A terminal value (also known as an intrinsic value) is an ultimate goal, an end-in-itself. ... In an [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) with a [utility](https://www.lesswrong.com/tag/utility-functions) or reward function, the terminal value is the maximization of that function.

Values are subjective but the question asks for some objective perspective. This question is of interest as “Humans' terminal values are often mutually contradictory, inconsistent, and changeable”.

Obviousness of natural selection (NS) can pose some constraints, albeit weak ones, as all known systems with sentient agents abide NS. But weak constraints are still better than no constraints at all.

Terminal goals are being split by natural selection into ones that fail to reproduce / maintain themselves and ones that survive (together with their bearers of cource). And sometimes we can even predict whether some terminal goals would go extinct or at least range their probability of survival (we already had put aside instrumental goals that “die” when they lose their purpose.).

So that's it. That's the only way to objectively judge terminal values I'm aware of. And judgment part comes from a feeling that I don't want to be invested in terminal goals that would most likely go extinct. At least they should be “mutated” in way to balance minimization of their change and maximization of their survival probability to be appealing.

*Are you aware of any other ways to evaluate terminal values?*

P.S. Basically, that post was a recap of a part of the more poetic and “old school” article that I've written: [Applying Universal Darwinism to evaluation of Terminal values](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md). The article doesn't add anything important to the question of this post but mostly stretches Universal Darwinism in other direstions instead.

----

## Buddha-Darwinism on objective meaning of life separated from subjective meaning of life (Cosmogonic myth from Darwinian natural selection, Quasi-immortality, Free will, Buddhism-like illusion of “Self”)

### aka Applying Universal Darwinism to evaluation of Terminal values gives “Buddarwinism”

----

## Do you see a way use abstract rewriting system over graphs from Wolfram Physics Project to represent code-data dual algothms that modify each other and form natural selection process?

Article [Some Relativistic and Gravitational Properties of the Wolfram
Model (by Jonathan Gorard - 2020)](https://www.wolframcloud.com/obj/wolframphysics/Documents/some-relativistic-and-gravitational-properties-of-the-wolfram-model.pdf) has interesting definition of abstract rewriting systems that work on graphs.

Do you see a way to use them to represent [code-data dual](https://en.wikipedia.org/wiki/Code_as_data) algorithms that modify each other and form natural selection process? That could be a nice base for open-ended natural selection a-life model.

More info:

* [Open-ended natural selection of interacting code-data-dual algorithms as a property analogous to Turing completeness](https://www.reddit.com/r/DigitalPhilosophy/comments/dzghec/openended_natural_selection_of_interacting/)
* [Wolfram Physics Project](https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/?source=frontpage-latest-news)

### [Comments](../comments/reddit_compsci_2.pdf)

----

## [Path to the Fundamental Theory of Physics by Stephen Wolfram (Wolfram Physics Project)](https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful)

### Comments

That's an interesting direction of research. Can it actually formaulate QFT in other postulates that are more computer science friendly? If not then how to bridge the enormous gap between simple rules + complex graph dynamics vs. QFT?

Possible workaround is to get a probabilistic model from simulation graph dynamics then run this new model as a simulation. But that would require emergence of "something" to abstact it in the new model. And if we are ever to automate this abstracting the fundamental-basic model and abstracted should share fractal structure (whatever that means).

In my opinion this way would better work with ontology of indeternimistic natural selection process that branches. Like [Open-ended natural selection of interacting code-data-dual algorithms as a property analogous to Turing completeness](https://www.reddit.com/r/DigitalPhilosophy/comments/dzghec/openended_natural_selection_of_interacting/) (and [more details with a discussion on r/compsci](https://www.reddit.com/r/compsci/comments/97s8dl/on_natural_selection_of_the_laws_of_nature/)).

So probably Wolfram Physics Project lacks indeterminism.

----

## The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities

https://arxiv.org/abs/1803.03453

Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.

Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J. Bentley, Samuel Bernard, Guillaume Beslon, David M. Bryson, Patryk Chrabaszcz, Nick Cheney, Antoine Cully, Stephane Doncieux, Fred C. Dyer, Kai Olav Ellefsen, Robert Feldt, Stephan Fischer, Stephanie Forrest, Antoine Frénoy, Christian Gagné, Leni Le Goff, Laura M. Grabowski, Babak Hodjat, Frank Hutter, Laurent Keller, Carole Knibbe, Peter Krcah, Richard E. Lenski, Hod Lipson, Robert MacCurdy, Carlos Maestre, Risto Miikkulainen, Sara Mitri, David E. Moriarty, Jean-Baptiste Mouret, Anh Nguyen, Charles Ofria, Marc Parizeau, David Parsons, Robert T. Pennock, William F. Punch, Thomas S. Ray, Marc Schoenauer, Eric Shulte, Karl Sims, Kenneth O. Stanley, François Taddei, Danesh Tarapore, Simon Thibault, Westley Weimer, Richard Watson, Jason Yosinski

----

## Computational complexity as an ultimate constraint on evolution

Artem Kaznatcheev

https://www.biorxiv.org/content/10.1101/187682v4

doi: https://doi.org/10.1101/187682

Abstract

Experiments show that evolutionary fitness landscapes can have a rich combinatorial structure due to epistasis. For some landscapes, this structure can produce a computational constraint that prevents evolution from finding local fitness optima -- thus overturning the traditional assumption that local fitness peaks can always be reached quickly if no other evolutionary forces challenge natural selection. Here, I introduce a distinction between easy landscapes of traditional theory where local fitness peaks can be found in a moderate number of steps and hard landscapes where finding local optima requires an infeasible amount of time. Hard examples exist even among landscapes with no reciprocal sign epistasis; on these semi-smooth fitness landscapes, strong selection weak mutation dynamics cannot find the unique peak in polynomial time. More generally, on hard rugged fitness landscapes that include reciprocal sign epistasis, no evolutionary dynamics -- even ones that do not follow adaptive paths -- can find a local fitness optimum quickly. Moreover, on hard landscapes, the fitness advantage of nearby mutants cannot drop off exponentially fast but has to follow a power-law that long term evolution experiments have associated with unbounded growth in fitness. Thus, the constraint of computational complexity enables open-ended evolution on finite landscapes. Knowing this constraint allows us to use the tools of theoretical computer science and combinatorial optimization to characterize the fitness landscapes that we expect to see in nature. I present candidates for hard landscapes at scales from single genes, to microbes, to complex organisms with costly learning (Baldwin effect) or maintained cooperation (Hankshaw effect). Just how ubiquitous hard landscapes (and the corresponding ultimate constraint on evolution) are in nature becomes an open empirical question.

### Comments

> Check out Donald Hoffman: https://www.youtube.com/watch?v=oYp5XuGYqqY&t=2s
>
> Great at explaining this stuff.

([u/eelson99](https://old.reddit.com/r/DigitalPhilosophy/comments/e1sk5r/computational_complexity_as_an_ultimate/frljvui/))

----

## [Introduction to Artificial Life for People who Like AI](https://thegradient.pub/an-introduction-to-artificial-life-for-people-who-like-ai/)

----

## [How the “bigger is better” mentality damages AI research](https://bdtechtalks.com/2019/11/25/ai-research-neural-networks-compute-costs/)

----

## Universal Darwinism as a process of Bayesian inference

John O. Campbell

https://arxiv.org/abs/1606.07937

(Submitted on 25 Jun 2016)

Many of the mathematical frameworks describing natural selection are equivalent to Bayes Theorem, also known as Bayesian updating. By definition, a process of Bayesian Inference is one which involves a Bayesian update, so we may conclude that these frameworks describe natural selection as a process of Bayesian inference. Thus natural selection serves as a counter example to a widely-held interpretation that restricts Bayesian Inference to human mental processes (including the endeavors of statisticians). As Bayesian inference can always be cast in terms of (variational) free energy minimization, natural selection can be viewed as comprising two components: a generative model of an "experiment" in the external world environment, and the results of that "experiment" or the "surprise" entailed by predicted and actual outcomes of the "experiment". Minimization of free energy implies that the implicit measure of "surprise" experienced serves to update the generative model in a Bayesian manner. This description closely accords with the mechanisms of generalized Darwinian process proposed both by Dawkins, in terms of replicators and vehicles, and Campbell, in terms of inferential systems. Bayesian inference is an algorithm for the accumulation of evidence-based knowledge. This algorithm is now seen to operate over a wide range of evolutionary processes, including natural selection, the evolution of mental models and cultural evolutionary processes, notably including science itself. The variational principle of free energy minimization may thus serve as a unifying mathematical framework for universal Darwinism, the study of evolutionary processes operating throughout nature.

----

## [The crisis in physics is not only about physics (by Sabine Hossenfelder)](http://backreaction.blogspot.com/2019/10/the-crisis-in-physics-is-not-only-about.html)

----

## Metaphysics is dead, long live the Applied Metaphysics! (on closing philosophical questions)

This article closes philosophical questions that bothered me for quite a long time. All what is left are science questions.

After writing [Are Universal Darwinism and Occam's razor enough to answer all Why? (Because of what?) questions?](https://www.reddit.com/r/DigitalPhilosophy/comments/9kdmll/are_universal_darwinism_and_occams_razor_enough/) article I finally understood what's the place of the Metaphysics in the modern Science.

Ancient metaphysical question **"Why is there something rather than nothing?"** is obviously answered **"It just is"** and obviously is reformulated into **"Why these structures exist instead of other structures?"**. I suppose the second question should be delegated to Science that should create a mathematical model of the Universe that is capable of answering all such questions. Our Universe should be **possible** in that model and existence of sentient life should be **probable** in such model. The model should be capable of giving predictions of the future (at it should be the very same model that gave explanations - not some ad hock addition). Let's call such a theory The Ultimate Theory (TUT) (like Douglas Adams's "The Ultimate Question of Life, the Universe, and Everything").

Mainstream physics is not eager to create such a theory and is just happy about [Grand Unified Theories](https://en.wikipedia.org/wiki/Grand_Unified_Theory) (GUT). For some reason they also call such theories [Theories of everything](https://en.wikipedia.org/wiki/Theory_of_everything) (ToE). But I fail to see how they are significantly different. There are theories from non-mainstream physics that are commonly also called Theories of Everything. As far as I know such theories are not capable of answering all such questions.

But what is the philosophical justification for The Ultimate Theory? How can it even claim to answer all **"Why these structures exist instead of other structures?"** questions? The answer is simple and as obvious as it can be. Let's assume that we have a theory that can answer all questions about reality. Such answers would either be postulates of the model or conclusions from the postulates. Conclusions part is obvious - that's exactly the meaning of "answering". But what about postulates? Why are they the way they are? And the obvious answer is **"They just are"** - we should start from something after all. If the theory is capable of answering all those questions then it's enough. That's our best idea about TUT. What if there would be another TUT? The one in which out Universe is more probable is better (assuming that they are equal in other aforementioned regards). If we would have several theories with equal probability of our Universe then they would constitute an equivalence class. And the objective part is abstracted this way. Like the notion of computability is abstracted in Turing completeness property or Gauge invariance to some constant ("Gauging away" as Lee Smolin called it).

So the two key ideas that close philosophical questions are:

* "They just are"
* Abstracting into single equivalence class all differences that are left

So what is left for metaphysics then? The good example of using metaphysical considerations in aid of creating of ToE is a [Temporal naturalism](http://arxiv.org/abs/1310.8539) article by Lee Smolin. There metaphysics ideas are used for creating a scientific theory (applied!).

Metaphysics is dead, long live the Applied Metaphysics!

*previous posts on topic are in [digital philosophy subreddit](https://www.reddit.com/r/DigitalPhilosophy/) (posts by kiwi0fruit)*

### Comments

#### 1.0

A bit [about applied metaphysics in Wikipedia](https://en.wikipedia.org/wiki/Metaphysics#Applied_metaphysics)

#### 2.0

> > Ancient metaphysical question **"Why is there something rather than nothing?"** is obviously answered **"It just is"** and obviously is reformulated into **"Why these structures exist instead of other structures?"**.
>
>These aren't good questions, or should I say, they aren't going to ever be answered.
>
>We can ask how *what we do have* came to be, but asking *what else could have been* is either extremely speculative or just nonsensical.
>
>I feel like there is a razor in here somewhere...... *"You cannot derive a **could have been** from an **is**." - The Mogget*
>
>There is only one reality we have access and it only can be what it can be, and we have no way of even speculating on what other realities or other options for our reality might be.

([ThMogget@Reddit](https://www.reddit.com/r/PhilosophyofScience/comments/e03ctn/comment/f8bxnnn))

#### 2.1

I do not agree but I find it plausible that lack of knowledge of what could have been would make future predicting models disjoint with explanation models of the past that operate on probabilities of events.

But may be there is still some hope for them not to be disjoint. Reminds me of [this](https://www.reddit.com/r/PhilosophyofScience/comments/dc6k9y/thus_one_possible_way_that_the_finetuning_problem/)

----
