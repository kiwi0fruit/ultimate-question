Evaluating terminal values
==========================

How to evaluate terminal values of humans (defined like in [here](https://www.lesswrong.com/tag/terminal-value))? Values are subjective but the question asks for some objective perspective.

Obviousness of natural selection (NS) can pose some constraints, albeit weak ones, as all known systems with sentient agents abide NS. But weak constraints are still better than no constraints at all.

Terminal goals are being split by natural selection into ones that fail to reproduce / maintain themselves and ones that survive (as we already put aside instrumental goals that “die” when they lose their purpose). And sometimes we can even predict whether some terminal goals would go extinct or at least range their probability of survival.

So that's it. That's the only way to objectively judge terminal values I'm aware of. And judgment part comes from a feeling that I don't want to be invested in terminal goals that would most likely go extinct. At least they should be “mutated” in way to balance minimization of their change and maximization of their survival probability to be appealing.

*Are you aware of any other ways to evaluate terminal values?*

P.S. Basically, that was a recap of a more poetic and “old school” article that I've written: [Applying Universal Darwinism to evaluation of goals and values gives Buddarwinism](https://github.com/kiwi0fruit/ultimate-question/blob/master/articles/dxb.md)  but it doesn't add anything important to the question of this post.
